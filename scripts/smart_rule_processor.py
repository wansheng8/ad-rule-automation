#!/usr/bin/env python3
"""
å¹¿å‘Šè§„åˆ™è‡ªåŠ¨åŒ–å¤„ç†ç³»ç»Ÿ - æœ€ç»ˆä¼˜åŒ–ç‰ˆ
GitHub Actionsä¸“ç”¨ï¼Œè§£å†³è¶…æ—¶å’Œæ¨é€é—®é¢˜
"""

import os
import sys
import re
import time
import json
import signal
import hashlib
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Set, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from config.settings import get_all_sources, Config
except ImportError as e:
    print(f"âŒ å¯¼å…¥é…ç½®å¤±è´¥: {e}")
    sys.exit(1)

# ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
DOMAIN_PATTERN = re.compile(r'^([a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]{2,}$')

# è¶…æ—¶æ§åˆ¶
class TimeoutException(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutException("å¤„ç†è¶…æ—¶")

signal.signal(signal.SIGALRM, timeout_handler)

def get_shanghai_time():
    try:
        return datetime.now(timezone(timedelta(hours=8)))
    except:
        return datetime.now()

def get_time_string():
    return get_shanghai_time().strftime('%Y-%m-%d %H:%M:%S')

class FastRuleFetcher:
    def __init__(self):
        try:
            import requests
            from requests.adapters import HTTPAdapter
            from urllib3.util.retry import Retry
            self.requests = requests
            self.HTTPAdapter = HTTPAdapter
            self.Retry = Retry
        except ImportError:
            print("âŒ è¯·å®‰è£…: pip install requests")
            sys.exit(1)
        
        self.session = self._create_session()
        self.cache_dir = Path(Config.CACHE_DIR)
        self.cache_dir.mkdir(exist_ok=True)
        
        self.stats = {
            'total': 0, 'success': 0, 'cached': 0,
            'failed': 0, 'timeout': 0
        }
    
    def _create_session(self):
        session = self.requests.Session()
        retry = self.Retry(total=1, backoff_factor=0.5)
        adapter = self.HTTPAdapter(
            max_retries=retry,
            pool_connections=5,
            pool_maxsize=5
        )
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        
        session.headers.update({
            'User-Agent': Config.get_user_agent(),
            'Accept': 'text/plain',
            'Accept-Encoding': 'gzip',
            'Connection': 'close'
        })
        
        return session
    
    def _get_cache_path(self, url: str) -> Path:
        url_hash = hashlib.md5(url.encode()).hexdigest()
        return self.cache_dir / f"cache_{url_hash}.txt"
    
    def fetch_url(self, url: str) -> Tuple[bool, Optional[str], int]:
        cache_file = self._get_cache_path(url)
        
        # æ£€æŸ¥ç¼“å­˜
        if Config.CACHE_ENABLED and cache_file.exists():
            cache_age = time.time() - cache_file.stat().st_mtime
            if cache_age < Config.CACHE_EXPIRE_HOURS * 3600:
                try:
                    with open(cache_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        lines = content.count('\n')
                        self.stats['cached'] += 1
                        self.stats['success'] += 1
                        return True, content, lines
                except:
                    pass
        
        # ç½‘ç»œè¯·æ±‚
        try:
            response = self.session.get(url, timeout=Config.REQUEST_TIMEOUT)
            response.raise_for_status()
            content = response.text
            lines = content.count('\n') + 1
            
            # ä¿å­˜ç¼“å­˜
            if Config.CACHE_ENABLED:
                try:
                    with open(cache_file, 'w', encoding='utf-8') as f:
                        f.write(content)
                except:
                    pass
            
            self.stats['success'] += 1
            return True, content, lines
            
        except Exception as e:
            self.stats['failed'] += 1
            return False, None, 0

class SimpleOptimizer:
    @staticmethod
    def deduplicate(rules: List[str]) -> List[str]:
        if not rules:
            return []
        
        # å¿«é€Ÿå»é‡
        seen = set()
        unique = []
        for rule in rules:
            if rule not in seen:
                seen.add(rule)
                unique.append(rule)
        
        # åŸŸåå»é‡ï¼ˆä»…å¯¹å¤§é‡æ•°æ®ï¼‰
        if len(unique) > 10000:
            domain_map = {}
            final = []
            for rule in unique:
                domain = None
                if rule.startswith('||') and '^' in rule:
                    domain = rule[2:].split('^')[0]
                elif rule.startswith(('0.0.0.0 ', '127.0.0.1 ')):
                    parts = rule.split()
                    if len(parts) >= 2:
                        domain = parts[1]
                
                if domain:
                    if domain not in domain_map:
                        domain_map[domain] = rule
                        final.append(rule)
                else:
                    final.append(rule)
            return final
        
        return unique
    
    @staticmethod
    def filter_rules(rules: List[str]) -> List[str]:
        if not rules:
            return []
        
        adblock = []
        hosts = []
        domains = []
        
        for rule in rules:
            if len(rule) > 500:
                continue
            
            if rule.startswith('||') or '##' in rule or rule.startswith('|'):
                adblock.append(rule)
            elif rule.startswith('0.0.0.0') or rule.startswith('127.0.0.1'):
                hosts.append(rule)
            elif DOMAIN_PATTERN.match(rule):
                domains.append(rule)
        
        result = []
        result.extend(sorted(adblock)[:Config.MAX_RULES_PER_TYPE])
        result.extend(sorted(hosts)[:Config.MAX_RULES_PER_TYPE//2])
        result.extend(sorted(domains)[:Config.MAX_RULES_PER_TYPE//2])
        
        return result[:Config.MAX_TOTAL_RULES]

class RuleProcessor:
    def __init__(self):
        self.fetcher = FastRuleFetcher()
        self.optimizer = SimpleOptimizer()
        self.all_rules = []
        
        try:
            sources = get_all_sources()
            self.sources = sources[:80] if len(sources) > 80 else sources
        except:
            self.sources = [
                "https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/adservers.txt",
                "https://easylist.to/easylist/easylist.txt",
                "https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts",
            ]
        
        self.start_time = time.time()
        self.stats = {
            'total_sources': len(self.sources),
            'total_rules': 0,
            'final_rules': 0,
            'duration': 0,
            'status': 'unknown'
        }
    
    def check_timeout(self):
        elapsed = time.time() - self.start_time
        if elapsed > Config.TIMEOUT_FORCE_STOP:
            print(f"â° è¶…æ—¶ä¿æŠ¤: {elapsed:.0f}ç§’")
            return True
        return False
    
    def process(self) -> bool:
        print("=" * 60)
        print("ğŸš€ å¹¿å‘Šè§„åˆ™å¤„ç†ç³»ç»Ÿ")
        print(f"ğŸ“… æ—¶é—´: {get_time_string()}")
        print(f"ğŸ“Š è§„åˆ™æº: {self.stats['total_sources']} ä¸ª")
        print("=" * 60)
        
        signal.alarm(Config.TIMEOUT_FORCE_STOP + 60)
        
        try:
            # 1. ä¸‹è½½
            print(f"\nğŸ“¥ ä¸‹è½½è§„åˆ™æº")
            contents = self._download_sources()
            if self.check_timeout():
                return False
            
            # 2. è§£æ
            print(f"\nğŸ” è§£æè§„åˆ™")
            self._parse_rules(contents)
            if self.check_timeout():
                return False
            
            # 3. ä¼˜åŒ–
            print(f"\nâš¡ ä¼˜åŒ–è§„åˆ™")
            final_rules = self._optimize_rules()
            if self.check_timeout():
                return False
            
            # 4. ä¿å­˜
            print(f"\nğŸ’¾ ä¿å­˜ç»“æœ")
            success = self._save_results(final_rules)
            
            self._generate_report(success)
            signal.alarm(0)
            return success
            
        except TimeoutException:
            print("\nâ° è¶…æ—¶ï¼Œä¿å­˜éƒ¨åˆ†ç»“æœ")
            self._save_partial()
            self.stats['status'] = 'timeout'
            return False
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")
            self.stats['status'] = 'error'
            return False
    
    def _download_sources(self) -> Dict[str, str]:
        contents = {}
        max_workers = min(Config.MAX_WORKERS, 4)
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(self.fetcher.fetch_url, url): url 
                      for url in self.sources}
            
            completed = 0
            for future in as_completed(futures):
                url = futures[future]
                success, content, lines = future.result()
                completed += 1
                
                if success and content:
                    contents[url] = content
                    if completed % 10 == 0:
                        print(f"  [{completed}/{len(self.sources)}] {lines} è¡Œ")
        
        print(f"âœ… ä¸‹è½½: {len(contents)}æˆåŠŸ, {self.fetcher.stats['failed']}å¤±è´¥")
        return contents
    
    def _parse_rules(self, contents: Dict[str, str]):
        count = 0
        for content in contents.values():
            lines = content.split('\n')
            for line in lines:
                line = line.strip()
                if not line or line[0] in '!#':
                    continue
                
                if len(line) < 500:
                    self.all_rules.append(line)
                    count += 1
                
                if count % 50000 == 0 and self.check_timeout():
                    return
        
        self.stats['total_rules'] = count
        print(f"âœ… è§£æ: {count:,} æ¡è§„åˆ™")
    
    def _optimize_rules(self) -> List[str]:
        print(f"  ä¼˜åŒ– {len(self.all_rules):,} æ¡è§„åˆ™...")
        
        # å»é‡
        unique = self.optimizer.deduplicate(self.all_rules)
        print(f"  å»é‡: {len(unique):,} æ¡")
        
        # è¿‡æ»¤
        final = self.optimizer.filter_rules(unique)
        self.stats['final_rules'] = len(final)
        print(f"  è¿‡æ»¤: {len(final):,} æ¡")
        
        return final
    
    def _save_results(self, rules: List[str]) -> bool:
        try:
            os.makedirs("dist", exist_ok=True)
            os.makedirs("stats", exist_ok=True)
            
            current_time = get_time_string()
            
            # åˆ†ç±»
            adblock = [r for r in rules if r.startswith('||') or '##' in r or r.startswith('|')]
            hosts = [r for r in rules if r.startswith('0.0.0.0') or r.startswith('127.0.0.1')]
            domains = [r for r in rules if DOMAIN_PATTERN.match(r)]
            
            # ä¿å­˜Adblock
            if adblock:
                with open("dist/Adblock.txt", 'w', encoding='utf-8') as f:
                    f.write(f"""! Adblockè§„åˆ™
! æ—¶é—´: {current_time}
! æ•°é‡: {len(adblock):,}
! é¡¹ç›®: https://github.com/{Config.REPO_OWNER}/{Config.REPO_NAME}
!

""")
                    for i in range(0, len(adblock), 10000):
                        f.write('\n'.join(adblock[i:i+10000]) + '\n')
                print(f"  âœ… Adblock: {len(adblock):,} æ¡")
            
            # ä¿å­˜Hosts
            if hosts:
                with open("dist/hosts.txt", 'w', encoding='utf-8') as f:
                    f.write(f"""# Hostsè§„åˆ™
# æ—¶é—´: {current_time}
# æ•°é‡: {len(hosts):,}
# é¡¹ç›®: https://github.com/{Config.REPO_OWNER}/{Config.REPO_NAME}
#

""")
                    for i in range(0, len(hosts), 10000):
                        f.write('\n'.join(hosts[i:i+10000]) + '\n')
                print(f"  âœ… Hosts: {len(hosts):,} æ¡")
            
            # ä¿å­˜åŸŸå
            if domains:
                with open("dist/Domains.txt", 'w', encoding='utf-8') as f:
                    f.write(f"""# åŸŸååˆ—è¡¨
# æ—¶é—´: {current_time}
# æ•°é‡: {len(domains):,}
# é¡¹ç›®: https://github.com/{Config.REPO_OWNER}/{Config.REPO_NAME}
#

""")
                    for domain in sorted(domains):
                        f.write(f"{domain}\n")
                print(f"  âœ… åŸŸå: {len(domains):,} ä¸ª")
            
            return True
            
        except Exception as e:
            print(f"  âŒ ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def _save_partial(self):
        try:
            if self.all_rules:
                sample = self.all_rules[:50000]
                optimized = self.optimizer.deduplicate(sample)
                
                os.makedirs("dist", exist_ok=True)
                with open("dist/Adblock_partial.txt", 'w', encoding='utf-8') as f:
                    f.write(f"! éƒ¨åˆ†è§„åˆ™ (è¶…æ—¶)\n")
                    f.write(f"! æ—¶é—´: {get_time_string()}\n")
                    f.write(f"! æ•°é‡: {len(optimized):,}\n!\n\n")
                    f.write('\n'.join(optimized[:20000]))
                
                print(f"  âš ï¸  ä¿å­˜éƒ¨åˆ†è§„åˆ™")
        except:
            pass
    
    def _generate_report(self, success: bool):
        try:
            elapsed = time.time() - self.start_time
            self.stats['duration'] = round(elapsed, 2)
            self.stats['status'] = 'success' if success else 'partial'
            
            report = {
                'timestamp': get_time_string(),
                'stats': self.stats,
                'fetcher_stats': self.fetcher.stats,
                'config': {
                    'max_workers': Config.MAX_WORKERS,
                    'timeout': Config.REQUEST_TIMEOUT,
                    'cache_enabled': Config.CACHE_ENABLED
                }
            }
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            with open(f"stats/report_{timestamp}.json", 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            print(f"\n{'='*60}")
            print(f"{'âœ… æˆåŠŸ' if success else 'âš ï¸  éƒ¨åˆ†å®Œæˆ'}")
            print(f"{'='*60}")
            print(f"â±ï¸  è€—æ—¶: {elapsed:.1f}ç§’")
            print(f"ğŸ“Š åŸå§‹: {self.stats['total_rules']:,} æ¡")
            print(f"ğŸ“Š æœ€ç»ˆ: {self.stats['final_rules']:,} æ¡")
            print(f"ğŸ“¥ ä¸‹è½½: {self.fetcher.stats['success']}æˆåŠŸ "
                  f"({self.fetcher.stats['cached']}ç¼“å­˜)")
            
        except Exception as e:
            print(f"  âš ï¸  æŠ¥å‘Šå¤±è´¥: {e}")

def main():
    print("ğŸ”„ å¯åŠ¨è§„åˆ™å¤„ç†")
    
    def interrupt_handler(sig, frame):
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    
    signal.signal(signal.SIGINT, interrupt_handler)
    
    try:
        processor = RuleProcessor()
        success = processor.process()
        return 0 if success else 1
        
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿé”™è¯¯: {e}")
        return 1
    finally:
        signal.alarm(0)

if __name__ == "__main__":
    sys.exit(main())
