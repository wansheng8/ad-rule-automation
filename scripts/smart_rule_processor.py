#!/usr/bin/env python3
"""
æ™ºèƒ½å¹¿å‘Šè§„åˆ™å¤„ç†ç³»ç»Ÿ v3.0
æ”¯æŒæ•°ç™¾æ¡è§„åˆ™æºçš„è‡ªåŠ¨è¯†åˆ«ã€ä¼˜åŒ–å’Œåˆå¹¶
"""

import os
import sys
import re
import yaml
import json
import time
import logging
import argparse
from datetime import datetime
from typing import Dict, List, Set, Tuple, Optional
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
except ImportError:
    print("é”™è¯¯ï¼šè¯·å…ˆå®‰è£…ä¾èµ–ï¼špip install requests pyyaml")
    sys.exit(1)

# å¯¼å…¥é…ç½®
try:
    from config.settings import Config, DEFAULT_RULE_SOURCES
except ImportError:
    # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    class Config:
        MAX_WORKERS = 15
        REQUEST_TIMEOUT = 30
        OUTPUT_DIR = "dist"
        STATS_DIR = "stats"
        BACKUP_DIR = "backups"

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('rule_processor.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class RuleFetcher:
    """è§„åˆ™è·å–å™¨ - æ”¯æŒé‡è¯•å’Œå¹¶å‘"""
    
    def __init__(self):
        self.session = self._create_session()
        self.success_count = 0
        self.failed_count = 0
        
    def _create_session(self):
        """åˆ›å»ºHTTPä¼šè¯ï¼Œé…ç½®é‡è¯•ç­–ç•¥"""
        session = requests.Session()
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "HEAD"]
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        # è®¾ç½®è¯·æ±‚å¤´
        session.headers.update({
            'User-Agent': 'AdRuleAutomation/1.0',
            'Accept': 'text/plain, application/octet-stream, */*',
            'Accept-Language': 'en-US,en;q=0.9',
        })
        
        return session
    
    def fetch_url(self, url: str) -> Optional[str]:
        """è·å–å•ä¸ªURLçš„å†…å®¹"""
        try:
            response = self.session.get(
                url, 
                timeout=Config.REQUEST_TIMEOUT,
                allow_redirects=True
            )
            response.raise_for_status()
            
            # æ£€æŸ¥å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '').lower()
            if 'text' not in content_type and 'octet-stream' not in content_type:
                logger.warning(f"URL {url} è¿”å›éæ–‡æœ¬å†…å®¹: {content_type}")
                return None
            
            self.success_count += 1
            return response.text
            
        except requests.exceptions.RequestException as e:
            logger.error(f"è·å–URLå¤±è´¥ {url}: {e}")
            self.failed_count += 1
            return None
    
    def fetch_batch(self, urls: List[str], max_workers: int = None) -> Dict[str, str]:
        """æ‰¹é‡è·å–URLå†…å®¹"""
        if max_workers is None:
            max_workers = Config.MAX_WORKERS
            
        results = {}
        
        logger.info(f"å¼€å§‹æ‰¹é‡è·å– {len(urls)} ä¸ªURLï¼Œå¹¶å‘æ•°: {max_workers}")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {executor.submit(self.fetch_url, url): url for url in urls}
            
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    content = future.result()
                    if content:
                        results[url] = content
                except Exception as e:
                    logger.error(f"å¤„ç†URLæ—¶å‡ºé”™ {url}: {e}")
        
        logger.info(f"è·å–å®Œæˆ: æˆåŠŸ {self.success_count}, å¤±è´¥ {self.failed_count}")
        return results

class RuleAnalyzer:
    """è§„åˆ™åˆ†æå™¨ - æ™ºèƒ½è¯†åˆ«å’Œåˆ†ç±»"""
    
    # è§„åˆ™ç±»å‹ç‰¹å¾
    PATTERNS = {
        'adblock': [
            (r'^\|\|.*\^$', 0.9),           # ||example.com^
            (r'^@@\|\|.*\^$', 0.8),         # @@||example.com^ (ç™½åå•)
            (r'^\|https?://.*\|$', 0.7),    # |http://example.com|
            (r'^/.*/\$.*', 0.6),            # /ads/*$domain=example.com
            (r'^##.*', 0.5),                # ##div[ad] (å…ƒç´ éšè—)
            (r'^#@#.*', 0.5),               # #@#div[ad] (ç™½åå•)
            (r'.*\$(script|image|stylesheet|object)', 0.8),  # èµ„æºç±»å‹
            (r'.*\.(gif|jpg|png|js|swf)\^.*', 0.7),  # æ–‡ä»¶æ‰©å±•å
        ],
        'hosts': [
            (r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\s+\S+', 1.0),  # IP åŸŸå
            (r'^127\.0\.0\.1\s+', 0.9),     # 127.0.0.1
            (r'^::1\s+', 0.8),              # IPv6
            (r'^#\s*Hosts\s*file', 0.7),    # Hostsæ–‡ä»¶æ³¨é‡Š
        ],
        'domain': [
            (r'^[a-zA-Z0-9][a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', 0.9),  # çº¯åŸŸå
            (r'^\*?\.[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', 0.8),        # *.example.com
        ],
        'regex': [
            (r'^/.*/$', 0.9),               # æ­£åˆ™è¡¨è¾¾å¼
            (r'.*\$.*', 0.6),               # å¸¦é€‰é¡¹çš„è§„åˆ™
        ]
    }
    
    @staticmethod
    def detect_type(line: str) -> Tuple[str, float]:
        """æ£€æµ‹å•è¡Œè§„åˆ™ç±»å‹åŠç½®ä¿¡åº¦"""
        line = line.strip()
        
        if not line or line.startswith('!') or line.startswith('#'):
            return 'comment', 1.0
        
        best_type = 'unknown'
        best_score = 0.0
        
        for rule_type, patterns in RuleAnalyzer.PATTERNS.items():
            for pattern, score in patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    if score > best_score:
                        best_score = score
                        best_type = rule_type
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°å·²çŸ¥æ¨¡å¼ï¼Œå°è¯•å¯å‘å¼åˆ¤æ–­
        if best_type == 'unknown':
            if '.' in line and len(line) > 4 and ' ' not in line:
                return 'domain', 0.5
        
        return best_type, best_score
    
    @staticmethod
    def extract_domain(rule: str) -> Optional[str]:
        """ä»è§„åˆ™ä¸­æå–åŸŸå"""
        # ç§»é™¤å¸¸è§å‰ç¼€
        prefixes = ['||', '|', '://', 'http://', 'https://', 'www.']
        for prefix in prefixes:
            if rule.startswith(prefix):
                rule = rule[len(prefix):]
        
        # ç§»é™¤å¸¸è§åç¼€
        suffixes = ['^', '^$', '/', '/*', '^$third-party']
        for suffix in suffixes:
            if rule.endswith(suffix):
                rule = rule[:-len(suffix)]
        
        # æå–åŸŸå
        domain_pattern = r'([a-zA-Z0-9][a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'
        match = re.search(domain_pattern, rule)
        
        if match:
            domain = match.group(1).lower()
            # éªŒè¯åŸŸå
            if RuleAnalyzer.is_valid_domain(domain):
                return domain
        
        return None
    
    @staticmethod
    def is_valid_domain(domain: str) -> bool:
        """éªŒè¯åŸŸåæœ‰æ•ˆæ€§"""
        if not domain or len(domain) < 4:
            return False
        
        # æ£€æŸ¥IPåœ°å€
        if re.match(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$', domain):
            return False
        
        # åŸŸåæ ¼å¼
        pattern = r'^[a-zA-Z0-9]([a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(\.[a-zA-Z0-9]([a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*\.[a-zA-Z]{2,}$'
        return bool(re.match(pattern, domain))

class RuleOptimizer:
    """è§„åˆ™ä¼˜åŒ–å™¨ - å»é‡å’Œæ’åº"""
    
    def __init__(self):
        self.rules = {
            'adblock': set(),
            'hosts': set(),
            'domains': set(),
            'regex': set(),
        }
        self.stats = {
            'total_processed': 0,
            'by_type': defaultdict(int),
            'duplicates_removed': 0,
        }
    
    def add_rule(self, rule: str, rule_type: str):
        """æ·»åŠ è§„åˆ™åˆ°å¯¹åº”é›†åˆ"""
        if rule_type in self.rules:
            if rule in self.rules[rule_type]:
                self.stats['duplicates_removed'] += 1
            else:
                self.rules[rule_type].add(rule)
                self.stats['by_type'][rule_type] += 1
        
        self.stats['total_processed'] += 1
    
    def optimize_adblock(self) -> List[str]:
        """ä¼˜åŒ–Adblockè§„åˆ™"""
        optimized = []
        seen_domains = set()
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        def get_priority(rule):
            priority = 50
            
            # å…³é”®è¯ä¼˜å…ˆçº§
            rule_lower = rule.lower()
            for keyword in ['ad', 'ads', 'track', 'analytics']:
                if keyword in rule_lower:
                    priority += 30
                    break
            
            # è§„åˆ™ç±»å‹ä¼˜å…ˆçº§
            if rule.startswith('||') and rule.endswith('^'):
                priority += 20
            elif rule.startswith('@@'):
                priority -= 10
            elif '##' in rule:
                priority += 10
            
            # é•¿åº¦ä¼˜å…ˆçº§ï¼ˆçŸ­çš„è§„åˆ™æ›´é€šç”¨ï¼‰
            if len(rule) < 20:
                priority += 10
            
            return priority
        
        # æ’åºå¹¶å»é‡
        sorted_rules = sorted(
            self.rules['adblock'],
            key=lambda x: (-get_priority(x), len(x), x)
        )
        
        for rule in sorted_rules:
            domain = RuleAnalyzer.extract_domain(rule)
            if domain and domain in seen_domains:
                # å¦‚æœå·²ç»æœ‰æ›´é€šç”¨çš„è§„åˆ™ï¼Œè·³è¿‡ç‰¹å®šè§„åˆ™
                if rule.startswith('||') and rule.endswith('^'):
                    continue
            
            if domain:
                seen_domains.add(domain)
            
            optimized.append(rule)
            
            # é™åˆ¶æ•°é‡
            if len(optimized) >= getattr(Config, 'MAX_RULES_PER_TYPE', 50000):
                break
        
        return optimized
    
    def optimize_hosts(self) -> Dict[str, List[str]]:
        """ä¼˜åŒ–Hostsè§„åˆ™ï¼ŒæŒ‰IPåˆ†ç»„"""
        hosts_dict = defaultdict(set)
        
        for entry in self.rules['hosts']:
            # è§£æHostsæ¡ç›®
            parts = entry.strip().split()
            if len(parts) >= 2:
                ip = parts[0]
                domain = parts[1]
                
                if ip in ['0.0.0.0', '127.0.0.1']:
                    if RuleAnalyzer.is_valid_domain(domain):
                        hosts_dict[ip].add(domain)
        
        # è½¬æ¢ä¸ºæ’åºåˆ—è¡¨
        return {
            ip: sorted(domains)
            for ip, domains in hosts_dict.items()
        }
    
    def optimize_domains(self) -> List[str]:
        """ä¼˜åŒ–åŸŸååˆ—è¡¨"""
        domains = sorted({
            domain for domain in self.rules['domains']
            if RuleAnalyzer.is_valid_domain(domain)
        })
        
        return domains[:getattr(Config, 'MAX_RULES_PER_TYPE', 50000)]

class SmartRuleProcessor:
    """æ™ºèƒ½è§„åˆ™å¤„ç†å™¨ - ä¸»ç±»"""
    
    def __init__(self, config_file: str = None):
        self.config_file = config_file
        self.fetcher = RuleFetcher()
        self.analyzer = RuleAnalyzer()
        self.optimizer = RuleOptimizer()
        
        # åŠ è½½è§„åˆ™æº
        self.rule_sources = self._load_rule_sources()
        
    def _load_rule_sources(self) -> List[str]:
        """åŠ è½½è§„åˆ™æºåˆ—è¡¨"""
        sources = []
        
        # 1. ä»YAMLæ–‡ä»¶åŠ è½½
        if self.config_file and os.path.exists(self.config_file):
            with open(self.config_file, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                if isinstance(data, list):
                    sources.extend(data)
        
        # 2. ä»TXTæ–‡ä»¶åŠ è½½
        txt_file = os.path.join('config', 'rule_sources.txt')
        if os.path.exists(txt_file):
            with open(txt_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        sources.append(line)
        
        # 3. ä½¿ç”¨é»˜è®¤è§„åˆ™æº
        if not sources:
            logger.warning("æœªæ‰¾åˆ°è§„åˆ™æºæ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤è§„åˆ™æº")
            sources = [
                "https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/adservers.txt",
                "https://easylist.to/easylist/easylist.txt",
                "https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts",
            ]
        
        # å»é‡
        return list(set(sources))
    
    def process(self):
        """ä¸»å¤„ç†æµç¨‹"""
        logger.info("å¼€å§‹æ™ºèƒ½è§„åˆ™å¤„ç†")
        start_time = time.time()
        
        # 1. è·å–è§„åˆ™æºå†…å®¹
        logger.info(f"è·å– {len(self.rule_sources)} ä¸ªè§„åˆ™æº")
        source_contents = self.fetcher.fetch_batch(self.rule_sources)
        
        # 2. åˆ†æå¤„ç†æ¯ä¸ªè§„åˆ™æº
        logger.info("åˆ†æè§„åˆ™å†…å®¹")
        for url, content in source_contents.items():
            self._process_content(content, url)
        
        # 3. ä¼˜åŒ–è§„åˆ™
        logger.info("ä¼˜åŒ–è§„åˆ™")
        adblock_rules = self.optimizer.optimize_adblock()
        hosts_rules = self.optimizer.optimize_hosts()
        
        # 4. ä¿å­˜ç»“æœ - åªä¿å­˜ä¸¤ä¸ªæ ¸å¿ƒæ–‡ä»¶
        logger.info("ä¿å­˜ç»“æœ")
        results = self._save_results(adblock_rules, hosts_rules)
        
        # 5. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        elapsed_time = time.time() - start_time
        self._generate_report(results, elapsed_time)
        
        return results
    
    def _process_content(self, content: str, source_url: str):
        """å¤„ç†å•ä¸ªè§„åˆ™æºå†…å®¹"""
        lines = content.split('\n')
        
        for line_num, line in enumerate(lines, 1):
            rule_type, confidence = self.analyzer.detect_type(line)
            
            if rule_type != 'comment' and confidence > 0.3:
                self.optimizer.add_rule(line.strip(), rule_type)
    
    def _save_results(self, adblock_rules, hosts_rules):
        """ä¿å­˜ä¼˜åŒ–åçš„è§„åˆ™ - åªç”Ÿæˆä¸¤ä¸ªå›ºå®šæ–‡ä»¶"""
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(Config.OUTPUT_DIR, exist_ok=True)
        os.makedirs(Config.STATS_DIR, exist_ok=True)
        
        results = {}
        
        # 1. ä¿å­˜Adblockè§„åˆ™ - å›ºå®šæ–‡ä»¶å
        adblock_file = os.path.join(Config.OUTPUT_DIR, "adblock_optimized.txt")
        
        with open(adblock_file, 'w', encoding='utf-8') as f:
            f.write(self._generate_header('Adblockè§„åˆ™', len(adblock_rules)))
            f.write('\n'.join(adblock_rules))
        
        results['adblock'] = adblock_file
        logger.info(f"ä¿å­˜Adblockè§„åˆ™: {len(adblock_rules)} æ¡")
        
        # 2. ä¿å­˜Hostsè§„åˆ™ - å›ºå®šæ–‡ä»¶å
        hosts_file = os.path.join(Config.OUTPUT_DIR, "hosts_optimized.txt")
        
        with open(hosts_file, 'w', encoding='utf-8') as f:
            f.write(self._generate_header('Hostsè§„åˆ™', 
                sum(len(d) for d in hosts_rules.values())))
            
            for ip, domains in hosts_rules.items():
                for domain in domains:
                    f.write(f"{ip} {domain}\n")
        
        results['hosts'] = hosts_file
        logger.info(f"ä¿å­˜Hostsè§„åˆ™: {sum(len(d) for d in hosts_rules.values())} æ¡")
        
        # 3. ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_file = os.path.join(
            Config.STATS_DIR, 
            f"stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        
        stats_data = {
            'timestamp': datetime.now().isoformat(),
            'rule_sources': len(self.rule_sources),
            'rules_processed': self.optimizer.stats['total_processed'],
            'rules_by_type': dict(self.optimizer.stats['by_type']),
            'duplicates_removed': self.optimizer.stats['duplicates_removed'],
            'output_counts': {
                'adblock': len(adblock_rules),
                'hosts_domains': sum(len(d) for d in hosts_rules.values()),
            },
            'performance': {
                'successful_fetches': self.fetcher.success_count,
                'failed_fetches': self.fetcher.failed_count,
            }
        }
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats_data, f, indent=2, ensure_ascii=False)
        
        results['stats'] = stats_file
        
        return results
    
    def _generate_header(self, title: str, count: int) -> str:
        """ç”Ÿæˆæ–‡ä»¶å¤´éƒ¨ä¿¡æ¯"""
        return f"""! {title}
! æœ€åæ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
! è§„åˆ™æ€»æ•°: {count}
! 
! ç”±æ™ºèƒ½å¹¿å‘Šè§„åˆ™å¤„ç†ç³»ç»Ÿç”Ÿæˆ
! GitHub: https://github.com/wansheng8/ad-rule-automation
! 
! ç»Ÿè®¡ä¿¡æ¯:
! - å¤„ç†è§„åˆ™æº: {len(self.rule_sources)} ä¸ª
! - Adblockè§„åˆ™: {len(self.optimizer.rules['adblock'])} æ¡
! - Hostsæ¡ç›®: {len(self.optimizer.rules['hosts'])} æ¡
! - å”¯ä¸€åŸŸå: {len(self.optimizer.rules['domains'])} ä¸ª
! - é‡å¤ç§»é™¤: {self.optimizer.stats['duplicates_removed']} æ¡
!

"""
    
    def _generate_report(self, results: Dict, elapsed_time: float):
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        report_file = os.path.join(
            Config.STATS_DIR, 
            f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        )
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"# è§„åˆ™å¤„ç†æŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write(f"## ğŸ“Š å¤„ç†ç»Ÿè®¡\n\n")
            f.write(f"- **æ€»è€—æ—¶**: {elapsed_time:.2f} ç§’\n")
            f.write(f"- **è§„åˆ™æºæ•°é‡**: {len(self.rule_sources)}\n")
            f.write(f"- **æˆåŠŸè·å–**: {self.fetcher.success_count}\n")
            f.write(f"- **å¤±è´¥è·å–**: {self.fetcher.failed_count}\n\n")
            
            f.write(f"## ğŸ“ˆ è§„åˆ™ç»Ÿè®¡\n\n")
            f.write(f"| è§„åˆ™ç±»å‹ | å¤„ç†æ•°é‡ | è¾“å‡ºæ•°é‡ |\n")
            f.write(f"|----------|----------|----------|\n")
            f.write(f"| Adblock | {self.optimizer.stats['by_type']['adblock']} | {len(self.optimizer.rules['adblock'])} |\n")
            f.write(f"| Hosts | {self.optimizer.stats['by_type']['hosts']} | {len(self.optimizer.rules['hosts'])} |\n")
            f.write(f"| åŸŸå | {self.optimizer.stats['by_type']['domain']} | {len(self.optimizer.rules['domains'])} |\n")
            f.write(f"| æ€»è®¡ | {self.optimizer.stats['total_processed']} | - |\n\n")
            
            f.write(f"## ğŸ’¾ è¾“å‡ºæ–‡ä»¶\n\n")
            f.write(f"- `adblock_optimized.txt` ({len(self.optimizer.rules['adblock'])} æ¡è§„åˆ™)\n")
            f.write(f"- `hosts_optimized.txt` ({len(self.optimizer.rules['hosts'])} æ¡è§„åˆ™)\n")
            
            f.write(f"\n## ğŸš€ ä½¿ç”¨è¯´æ˜\n\n")
            f.write(f"1. **Adblockè§„åˆ™**: é€‚ç”¨äºuBlock Originã€AdGuardç­‰æµè§ˆå™¨æ‰©å±•\n")
            f.write(f"2. **Hostsè§„åˆ™**: å¤åˆ¶åˆ°ç³»ç»Ÿhostsæ–‡ä»¶ï¼ˆéœ€è¦ç®¡ç†å‘˜æƒé™ï¼‰\n")
        
        logger.info(f"æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description='æ™ºèƒ½å¹¿å‘Šè§„åˆ™å¤„ç†ç³»ç»Ÿ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  %(prog)s                            # ä½¿ç”¨é»˜è®¤é…ç½®
  %(prog)s --config rule_sources.yaml # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
  %(prog)s --test                     # æµ‹è¯•æ¨¡å¼ï¼ˆä»…å¤„ç†å°‘é‡è§„åˆ™æºï¼‰
        """
    )
    
    parser.add_argument(
        '--config', 
        type=str, 
        default='config/rule_sources.yaml',
        help='è§„åˆ™æºé…ç½®æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--test',
        action='store_true',
        help='æµ‹è¯•æ¨¡å¼ï¼ˆä»…å¤„ç†å‰5ä¸ªè§„åˆ™æºï¼‰'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default='dist',
        help='è¾“å‡ºç›®å½•'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='è¯¦ç»†è¾“å‡ºæ¨¡å¼'
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logger.setLevel(logging.DEBUG)
    
    # æ›´æ–°é…ç½®
    Config.OUTPUT_DIR = args.output
    
    try:
        processor = SmartRuleProcessor(args.config)
        
        # æµ‹è¯•æ¨¡å¼ï¼šä»…å¤„ç†å‰5ä¸ªè§„åˆ™æº
        if args.test:
            logger.info("è¿è¡Œæµ‹è¯•æ¨¡å¼")
            processor.rule_sources = processor.rule_sources[:5]
        
        # è¿è¡Œå¤„ç†
        results = processor.process()
        
        # è¾“å‡ºç»“æœè·¯å¾„
        print("\n" + "="*60)
        print("å¤„ç†å®Œæˆï¼")
        print("="*60)
        print(f"âœ“ Adblockè§„åˆ™: adblock_optimized.txt ({len(processor.optimizer.rules['adblock'])} æ¡)")
        print(f"âœ“ Hostsè§„åˆ™: hosts_optimized.txt ({len(processor.optimizer.rules['hosts'])} æ¡)")
        
        return 0
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­å¤„ç†")
        return 130
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {e}", exc_info=True)
        return 1

if __name__ == "__main__":
    sys.exit(main())
